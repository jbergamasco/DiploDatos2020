{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"},"colab":{"name":"3- Introducción a Aprendizaje Automático y Aprendizaje Automático Supervisado - Precios Claros.ipynb","provenance":[],"collapsed_sections":["X81BF0w7LDK4","jgKY95IMLDK4","vThawnmoLDK5"],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"L_ybtpSKLDKB","colab_type":"text"},"source":["<center>\n","<h4>Diplomatura en CDAAyA 2020 - FaMAF - UNC</h4>\n","<h1>¿Caro o Barato? Análisis de Precios de Almacen en un Contexto Inflacionario</h1>\n","<h3>Introducción al Aprendizaje Automático & Aprendizaje Automático Supervisado</h3>\n","</center>\n","</left>\n","<h4>Sofía Luján y Julieta Bergamasco</h4>\n","</left>"]},{"cell_type":"markdown","metadata":{"id":"rfSQrD--LDKC","colab_type":"text"},"source":["## Introducción\n","\n","En la siguiente notebook se presentará la consigna a seguir para el tercer práctico del proyecto, correspondiente a las materias Introducción al Aprendizaje Automático y Aprendizaje Automático Supervisado. El objetivo consiste en explorar la aplicación de diferentes métodos de aprendizaje supervisado aprendidos en el curso, así como también de métodos de _ensemble learning_, a través de experimentos reproducibles, y evaluando a su vez la conveniencia de uno u otro, así como la selección de diferentes hiperparámetros a partir del cálculo de las métricas pertinentes.\n","\n","En el caso de nuestro proyecto, podemos plantear diferentes tipos de problemas, como la agrupación de productos, la estimación de un precio o la identificación de precios anómalos. Sin embargo, a los fines de este práctico, nos enfocaremos en la predicción de precios relativos.\n","\n","Para ello, comenzaremos con las importaciones pertinentes."]},{"cell_type":"markdown","metadata":{"id":"ZvbVaP89LDKC","colab_type":"text"},"source":["## Importaciones"]},{"cell_type":"code","metadata":{"id":"XufxRC-ULDKD","colab_type":"code","colab":{}},"source":["# Importación de las librerías necesarias\n","import numpy as np\n","import pandas as pd\n","# Puede que nos sirvan también\n","import matplotlib as mpl\n","mpl.get_cachedir()\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import seaborn as sns\n","import plotly.express as px\n","import sklearn as skl\n","from io import StringIO\n","\n","from sklearn import preprocessing\n","from sklearn.utils import shuffle, print_eval\n","from sklearn.metrics import accuracy_score, confusion_matrix, mean_squared_error, classification_report, roc_curve, auc\n","from sklearn import ensemble #RandomForestClasifier, VotingClassifier\n","from sklearn import svm #LinearSVC, SVC\n","from sklearn import neural_network\n","from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n","\n","np.random.seed(0)  # Para mayor determinismo"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D5hy3FdoLDKG","colab_type":"code","colab":{}},"source":["pd.set_option('display.max_columns', 150)\n","pd.set_option('display.max_rows', 150)\n","pd.set_option('max_colwidth', 151)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p_aubegOLDKI","colab_type":"text"},"source":["## Consigna"]},{"cell_type":"markdown","metadata":{"id":"HK5vvq1aLDKJ","colab_type":"text"},"source":["### I. Preprocesamiento\n","\n","A los fines de realizar este práctico, se utilizará el dataset limpio obtenido en la etapa anterior. La división entre train y test será realizada en este mismo práctico.\n","A continuación se detallan los pasos a seguir para el preprocesamiento de los datos.\n","\n","#### 1. Obtención del Dataset\n","\n","Cargar los datasets originales.\n","\n","#### 2. Aplicar Script de Curación\n","\n","Inicialmente, con el objetivo de preparar los datos que alimentarán los modelos de aprendizaje automático (ML) propuestos, deberán aplicar el script de curación obtenido en el práctico anterior.\n","En esta etapa, pueden adicionar los atributos que crean pertinentes a priori o que hayan encontrado interesantes por tener mayor correlación con la variable de interés (precio, precio relativo).\n","\n","#### 3. Correlación Entre Variables Numéricas\n","\n","Dado que inicialmente eran pocas las variables numéricas y ahora contamos con un grupo más amplio de estas caracteristicas, se propone obtener la correlación entre todas las variables numéricas. Representarla gráficamente utilizando un mapa de calor (heatmap).\n","¿Cuáles son las features más correlacionadas con el precio?\n","\n","#### 4. Multicolinealidad Exacta\n","\n","Las variables explicativas no deben estar muy correlacionadas entre ellas, ya que la variabilidad de una y otra estarán explicando la misma parte de variabilidad de la variable dependiente. Esto es lo que se conoce como multicolinealidad, lo cual deriva en la imposibilidad de estimar los parámetros cuando la misma es exacta o en estimaciones muy imprecisas cuando la misma es aproximada.\n","En el caso de encontrar multicolinealidad, responder: ¿Cómo se puede solucionar? ¿Qué decisión tomarían al respecto?\n","\n","#### 5. Normalización de Atributos\n","\n","Es posible que sea necesario normalizar las features de nuestro dataset, dado que muchos de los algoritmos de aprendizaje supervisado lo requieren. ¿En qué casos tendrá que implementarse normalización?\n","\n","Aplicar a los datasets la normalización de atributos que consideren adecuada.\n","\n","#### 6. Mezca Aleatória y División en Train/Test\n","\n","Finalmente, están en condiciones de **dividir el dataset en Train y Test**, utilizando para este último conjunto un 20% de los datos disponibles. Previo a esta división, es recomendable que mezclen los datos aleatoriamente.\n","De este modo, deberán obtener cuatro conjuntos de datos, para cada uno de los datasets: ```X_train```, ```X_test```, ```y_train``` y ```y_test```.\n","\n","Pensar si hacer de esta forma la división puede afectar la distribución espacial y temporal de los datos.\n","¿Cuáles pueden ser las consecuencias?\n","\n","\n","---\n","\n","A modo de ayuda, **en esta notebook encontrarán una especie de template** que sigue los pasos propuestos y que deberán ir completando.\n","\n","Recuerden que la ciencia de datos es un **proceso circular, continuo y no lineal**. Es decir, si los datos requieren de mayor procesamiento para satisfacer las necesidades de algoritmos de ML (cualesquiera de ellos), vamos a volver a la etapa inicial para, por ejemplo, crear nuevas features, tomar decisiones diferentes sobre valores faltantes o valores atípicos (outliers), descartar features, entre otras."]},{"cell_type":"markdown","metadata":{"id":"MGTKH-QHLDKJ","colab_type":"text"},"source":["### II. Aplicación de Modelos de Aprendizaje Supervisado\n","\n","Una vez finalizada la etapa de preprocesamiento, se propone implementar diferentes modelos de predicción para el precio relativo, utilizando la librería Scikit-Learn:\n","\n","1. Linear Support Vector Regression ([Doc](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVR.html#sklearn.svm.LinearSVR))\n","2. Stochastic Gradient Descent ([Doc](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor))\n","3. Regression Based on k-nearest neighbors ([Doc](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html#sklearn.neighbors.KNeighborsRegressor))\n","4. Gaussian Process Regression ([Doc](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessRegressor.html#sklearn.gaussian_process.GaussianProcessRegressor))\n","5. Prediction Voting Regressor ([Doc](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingRegressor.html#sklearn.ensemble.VotingRegressor))\n","\n","Para cada uno de ellos, se pide responder las siguientes consignas:\n","- Agregar vector de Bias, cuando lo crean pertinente. Cuándo hace falta y cuándo no? Por qué?\n","- Obtener MSE, MAE, RMSE, R Square\n","\n","De estos tipos de modelos, cuál creen que es el más adecuado para nuestro caso de aplicación?\n","\n","**Elegir el modelo que consideren que mejor aplica a nuestro problema.** Para ello, recuerden que los pasos a seguir en la selección pueden esquematizarse como sigue:\n","\n","#### 1. Descripción de la Hipótesis\n","\n","¿Cuál es nuestro problema? ¿Cómo se caracteriza? ¿Cuál es la hipótesis?\n","\n","#### 2. Selección de Regularizador\n","\n"," ¿Utilizarán algún regularizador?¿Cuál?\n","\n","#### 3. Selección de Función de Costo\n","\n","¿Cuál será la función de costo utilizada?\n","\n","#### 4. Justificación de las Selecciones\n","\n","¿Por qué eligieron el modelo, el regularizador y la función de costo previas?\n","\n","Finalmente, para el modelo selecionado:\n","\n","- Utilizar el método *Grid Search*, o de búsqueda exahustiva, con *cross-validation* para profundizar en la búsqueda y selección de hiperparámetros.\n","- Calcular métricas sobre el conjunto de entrenamiento y de evaluación para los mejores parámetros obtenidos:\n","    + MSE, MAE, RMSE, R Square\n","    + Comparar las métricas obtenidas en cada modelo y obtener conclusiones.\n","\n","---\n","\n","Si encuentran cualquier otro modelo que consideren apropiado y deseen aplicar, pueden hacerlo con total libertad.\n","\n","**Opcional**\n","- Aplicar PCA para reducción de dimensionalidad (manteniendo *n* componentes principales) y entrenar nuevamente el modelo seleccionado como el más apropiado."]},{"cell_type":"markdown","metadata":{"id":"tr9iFhweLDKK","colab_type":"text"},"source":["### Entregables\n","\n","El entregable de este práctico consiste en **esta misma Notebook**, pero con el preprocesamiento aplicado y los modelos implementados, agregando las explicaciones que crean pertinentes y las decisiones tomadas, en caso de corresponder.\n","\n","Sintetizar las conclusiones en un archivo de texto, como lo vienen haciendo con los anteriores prácticos.\n","\n","**Fecha de Entrega: 31/08**"]},{"cell_type":"markdown","metadata":{"id":"XUwWIAKzLDKL","colab_type":"text"},"source":["# Resolución"]},{"cell_type":"markdown","metadata":{"id":"bzVdZgYCLDKL","colab_type":"text"},"source":["## I. Preprocesamiento"]},{"cell_type":"markdown","metadata":{"id":"QMK3WXbu2y_1","colab_type":"text"},"source":["### 1. Carga de Datos"]},{"cell_type":"markdown","metadata":{"id":"OryxZEl_LDKM","colab_type":"text"},"source":["Para comenzar, importamos los datos que vamos a utilizar:"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"78tfksdnLDKM","colab_type":"code","colab":{}},"source":["precios_20200412_20200413 = pd.read_csv('https://raw.githubusercontent.com/solujan/mentoria_2020/master/raw_dataset/precios_20200412_20200413.csv')\n","precios_20200419_20200419 = pd.read_csv('https://raw.githubusercontent.com/solujan/mentoria_2020/master/raw_dataset/precios_20200419_20200419.csv')\n","precios_20200426_20200426 = pd.read_csv('https://raw.githubusercontent.com/solujan/mentoria_2020/master/raw_dataset/precios_20200426_20200426.csv')\n","precios_20200502_20200503 = pd.read_csv('https://raw.githubusercontent.com/solujan/mentoria_2020/master/raw_dataset/precios_20200502_20200503.csv')\n","precios_20200518_20200518 = pd.read_csv('https://raw.githubusercontent.com/solujan/mentoria_2020/master/raw_dataset/precios_20200518_20200518.csv')\n","productos = pd.read_csv('https://raw.githubusercontent.com/solujan/mentoria_2020/master/raw_dataset/productos.csv')\n","sucursales = pd.read_csv('https://raw.githubusercontent.com/solujan/mentoria_2020/master/raw_dataset/sucursales.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"swMKQULvhRgR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1597250021232,"user_tz":180,"elapsed":776,"user":{"displayName":"sofia lujan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhRjGY1pGqEbZ6NzzBty0QMgutVKtWpvUr5f7ch8Q=s64","userId":"04644358706352499859"}},"outputId":"5aa739cc-0704-46be-bdda-dcd26d77fba8"},"source":["precios_20200412_20200413.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>precio</th>\n","      <th>producto_id</th>\n","      <th>sucursal_id</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>29.90</td>\n","      <td>0000000001663</td>\n","      <td>2-1-014</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>29.90</td>\n","      <td>0000000002288</td>\n","      <td>2-1-032</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>39.90</td>\n","      <td>0000000002288</td>\n","      <td>2-1-096</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>499.99</td>\n","      <td>0000000205870</td>\n","      <td>9-1-686</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>519.99</td>\n","      <td>0000000205870</td>\n","      <td>9-2-248</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   precio    producto_id sucursal_id\n","0   29.90  0000000001663     2-1-014\n","1   29.90  0000000002288     2-1-032\n","2   39.90  0000000002288     2-1-096\n","3  499.99  0000000205870     9-1-686\n","4  519.99  0000000205870     9-2-248"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"bhWrvNzALDKQ","colab_type":"text"},"source":["<div class=\"alert alert-block alert-info\">\n","El dataset ya está **listo para trabajar!**\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"3TNZgK3lLDKQ","colab_type":"text"},"source":["### 2. Aplicar Script de Curación"]},{"cell_type":"markdown","metadata":{"id":"nl6xwRWALDKQ","colab_type":"text"},"source":["El siguiente paso implica aplicar el script que resultó del práctico anterior. También pueden adicionar campos calculados en base a otros atributos, según lo consideren pertinente."]},{"cell_type":"code","metadata":{"id":"ILq6lPCTLDKR","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lSx4stUyLDKT","colab_type":"text"},"source":["### 3. Correlación Entre Variables Numéricas"]},{"cell_type":"code","metadata":{"id":"Ws9vOT22LDKU","colab_type":"code","colab":{}},"source":["# Hints: pueden usar df.corr() y sns.heatmap()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r_kgTmrBLDKW","colab_type":"text"},"source":["### 4. Multicolinealidad Exacta"]},{"cell_type":"markdown","metadata":{"id":"-BzB7hLTLDKX","colab_type":"text"},"source":["¿Existe multicolinealidad en nuestro dataset? ¿Cómo podemos saberlo?"]},{"cell_type":"code","metadata":{"id":"r1CY_3x9LDKX","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D853QGHaLDKd","colab_type":"text"},"source":["### 5. Normalización de Atributos\n"]},{"cell_type":"markdown","metadata":{"id":"S82eqotlLDKd","colab_type":"text"},"source":["Aplicar al dataset la normalización de atributos que consideren adecuada."]},{"cell_type":"code","metadata":{"id":"Ko03D4v7LDKd","colab_type":"code","colab":{}},"source":["# Pueden utilizar los siguientes métodos, por ejemplo:\n","\n","min_max_scaler = preprocessing.MinMaxScaler()\n","standard_scaler = preprocessing.StandardScaler()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1S-QIm6wLDKg","colab_type":"text"},"source":["### 7. Mezca Aleatória y División en Train/Test\n","\n","Primeramente, deberán mezclar los datos aleatoriamente. Luego, para dividir en Train/Test el dataset, aplicar el split utilizando un 20% de datos para este último.\n","\n","En este punto, deberán obtener cuatro conjuntos de datos, para ambos datasets: ```X_train```, ```X_test```, ```y_train``` y ```y_test```."]},{"cell_type":"code","metadata":{"id":"SNRr_mDBLDKh","colab_type":"code","colab":{}},"source":["# Para dividir el dataset, utilizar el siguiente módulo:\n","\n","_ds_shuff = shuffle(_ds)\n","\n","# Y luego el módulo:\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n","\n","# Notar que X e y son np.arrays. Además, pueden usar el parámetro que incluye train_test_split para mezclar."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kBfYvi4sLDKj","colab_type":"text"},"source":["## II. Aplicación de Modelos de Regresión"]},{"cell_type":"markdown","metadata":{"id":"b-C_K9c1LDKl","colab_type":"text"},"source":["Utilizando los datos de train y test obtenidos, se aplicarán diferentes modelos de regresión para predecir el precio relativo."]},{"cell_type":"markdown","metadata":{"id":"JK1h8ZiGLDKm","colab_type":"text"},"source":["### 1. Linear Support Vector Regression ([Doc](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVR.html#sklearn.svm.LinearSVR))"]},{"cell_type":"markdown","metadata":{"id":"dketRgbTLDKm","colab_type":"text"},"source":["A continuación se aplicará el modelo"]},{"cell_type":"code","metadata":{"id":"ls6owpdtT7Hv","colab_type":"code","colab":{}},"source":["from sklearn.svm import LinearSVR\n","from sklearn.metrics import mean_absolute_error "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LSs2gxOdLDKm","colab_type":"code","colab":{}},"source":["# En principio, pueden utilizar el módulo que sigue, con los parámetros por defecto y los que definan a continuación:\n","\n","eps = \n","c_value = \n","\n","svr = LinearSVR(epsilon=eps, C=c_value, fit_intercept=True)\n","svr.fit(X_train, y_train)\n","svr_results(y_test, X_test, svr)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Uk-p6fjYLDKp","colab_type":"code","colab":{}},"source":["def svr_results(y_test, X_test, fitted_svr_model):\n","    \n","    print(\"C: {}\".format(fitted_svr_model.C))\n","    print(\"Epsilon: {}\".format(fitted_svr_model.epsilon))\n","    \n","    print(\"Intercept: {:,.3f}\".format(fitted_svr_model.intercept_[0]))\n","    print(\"Coefficient: {:,.3f}\".format(fitted_svr_model.coef_[0]))\n","    \n","    mae = mean_absolute_error(y_test, fitted_svr_model.predict(X_test))\n","    \n","    print(\"MAE = ${:,.2f}\".format(1000*mae))\n","    \n","    perc_within_eps = 100*np.sum(y_test - fitted_svr_model.predict(X_test) < eps) / len(y_test)\n","    print(\"Percentage within Epsilon = {:,.2f}%\".format(perc_within_eps))\n","    \n","    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aUSNmqiALDKr","colab_type":"text"},"source":["### 2. Stochastic Gradient Descent ([Doc](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor))"]},{"cell_type":"code","metadata":{"id":"RJITHlV9LDKs","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1597231115247,"user_tz":180,"elapsed":930,"user":{"displayName":"sofia lujan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhRjGY1pGqEbZ6NzzBty0QMgutVKtWpvUr5f7ch8Q=s64","userId":"04644358706352499859"}},"outputId":"08691c63-0964-40b4-a4c8-e121bf204339"},"source":["import numpy as np\n","from sklearn.linear_model import SGDRegressor\n","from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","\n","# Always scale the input. The most convenient way is to use a pipeline.\n","\n","max_iter = \n","tol = \n","reg = make_pipeline(SGDRegressor(max_iter=max_iter, tol=tol))\n","reg.fit(X_train, y_train)\n","\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Pipeline(memory=None,\n","         steps=[('sgdregressor',\n","                 SGDRegressor(alpha=0.0001, average=False, early_stopping=False,\n","                              epsilon=0.1, eta0=0.01, fit_intercept=True,\n","                              l1_ratio=0.15, learning_rate='invscaling',\n","                              loss='squared_loss', max_iter=1000,\n","                              n_iter_no_change=5, penalty='l2', power_t=0.25,\n","                              random_state=None, shuffle=True, tol=0.001,\n","                              validation_fraction=0.1, verbose=0,\n","                              warm_start=False))],\n","         verbose=False)"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"4t4XzmiVZqgl","colab_type":"code","colab":{}},"source":["y_predict = reg.predict(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t3YxYOOOZUOV","colab_type":"code","colab":{}},"source":["mae = mean_absolute_error(y_test, y_predict)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rvXOH523chjx","colab_type":"text"},"source":["### 3. Regression Based on K-Nearest Neighbors ([Doc](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html#sklearn.neighbors.KNeighborsRegressor))"]},{"cell_type":"code","metadata":{"id":"qml8vv1EdCtk","colab_type":"code","colab":{}},"source":["from sklearn.neighbors import KNeighborsRegressor"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PiDToHHTdBNc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1597232249366,"user_tz":180,"elapsed":706,"user":{"displayName":"sofia lujan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhRjGY1pGqEbZ6NzzBty0QMgutVKtWpvUr5f7ch8Q=s64","userId":"04644358706352499859"}},"outputId":"f6d043d8-efad-4163-89e2-8fba91ac187b"},"source":["n_neighbors = 2\n","weights = 'uniform'\n","algorithm = 'auto'\n","\n","neigh = KNeighborsRegressor(n_neighbors=n_neighbors, weights= weights, algorithm=algorithm)\n","neigh.fit(X_train, y_train)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n","                    metric_params=None, n_jobs=None, n_neighbors=2, p=2,\n","                    weights='uniform')"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"HajWQrBTdp4e","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597232327978,"user_tz":180,"elapsed":888,"user":{"displayName":"sofia lujan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhRjGY1pGqEbZ6NzzBty0QMgutVKtWpvUr5f7ch8Q=s64","userId":"04644358706352499859"}},"outputId":"fc78b153-e97b-4add-b8ea-3a74d80e4797"},"source":["y_predict = neigh.predict(X_test)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.])"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"c2tlISQjcpZZ","colab_type":"text"},"source":["### 4. Gaussian Process Regression ([Doc](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessRegressor.html#sklearn.gaussian_process.GaussianProcessRegressor))"]},{"cell_type":"code","metadata":{"id":"GHulj5J_FHET","colab_type":"code","colab":{}},"source":["from klearn.gaussian_process import GaussianProcessRegressor\n","from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4aZ2CHL5QeHe","colab_type":"code","colab":{}},"source":["np.random.seed(1)\n","\n","# Instantiate a Gaussian Process model\n","kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2)) #If None is passed, the kernel “1.0 * RBF(1.0)” is used as default. Note that the kernel’s hyperparameters are optimized during fitting.\n","gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)\n","\n","# Fit to data using Maximum Likelihood Estimation of the parameters\n","gp.fit(X, y)\n","\n","# Make the prediction on the meshed x-axis (ask for MSE as well)\n","y_pred, sigma = gp.predict(x, return_std=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4GDIx0lmcpsK","colab_type":"text"},"source":["### 5. Prediction Voting Regressor ([Doc](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingRegressor.html#sklearn.ensemble.VotingRegressor))"]},{"cell_type":"code","metadata":{"id":"9C-3BkNyFGlv","colab_type":"code","colab":{}},"source":["from sklearn.linear_model import LinearRegression\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.ensemble import VotingRegressor\n","\n","r1 = LinearRegression()\n","r2 = RandomForestRegressor(n_estimators=10, random_state=1)\n","voting = VotingRegressor([('linear', r1), ('rf', r2)])\n","voting.fit(X_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kI9Ha_xELDK3","colab_type":"text"},"source":["### 4. Selección del Modelo"]},{"cell_type":"markdown","metadata":{"id":"X81BF0w7LDK4","colab_type":"text"},"source":["#### 4.1. Selección y Descripción de Hipótesis\n","\n","Describir el problema y la hipótesis del modelo."]},{"cell_type":"markdown","metadata":{"id":"jgKY95IMLDK4","colab_type":"text"},"source":["#### 4.2. Selección de Regularizador\n","\n"," ¿Utilizarán algún regularizador?¿Cuál?"]},{"cell_type":"markdown","metadata":{"id":"vThawnmoLDK5","colab_type":"text"},"source":["#### 4.3. Selección de Función de Costo\n","\n","¿Cuál será la función de costo utilizada?"]},{"cell_type":"markdown","metadata":{"id":"5zYHtgwjLDK5","colab_type":"text"},"source":["#### 4.4. Justificación de las Selecciones\n","\n","A continuación, se justifican las elecciones previas."]},{"cell_type":"markdown","metadata":{"id":"a5PCXv7OLDK5","colab_type":"text"},"source":["### 5. Selección de Parámetros y Métricas Sobre el Conjunto de Evaluación"]},{"cell_type":"markdown","metadata":{"id":"K9H4t7ZSLDK6","colab_type":"text"},"source":["Para la selección de hiperparámetros, pueden utilizar GridSearch. Además, deben calcular las métricas solicitadas."]},{"cell_type":"code","metadata":{"id":"IF3dTvgELDK6","colab_type":"code","colab":{}},"source":["# Para la búsqueda de los mejores parámetros, por ejemplo de Linear Support Vector Regression, pueden usar:\n","\n","exploring_params = {\n","        'epsilon': [0.5, 0], # \n","        'C': [1, 10, 100],  # Regularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive.\n","        'max_iter': [1000, 5000, 10000]   # The maximum number of iterations to be run.\n","    }\n","\n","m = LinearSVR(epsilon, C, max_iter, fit_intercept=True)\n","n_cross_val =   # Seleccionar folds\n","scoring = 'neg_mean_squared_error' # https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n","model = GridSearchCV(m, exploring_params, cv=n_cross_val, scoring=scoring)\n","    model.fit(X_train, y_train)\n","    \n","model.fit(X_train, y_train)\n","\n","print(\"Mejor conjunto de parámetros:\")\n","print(model.best_params_, end=\"\\n\\n\")\n","print()\n","print(\"Puntajes de la grilla:\", end=\"\\n\\n\")\n","means = model.cv_results_['mean_test_score']\n","stds = model.cv_results_['std_test_score'])\n","print()\n","print(\"================================================\", end=\"\\n\\n\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WnWfiAoYvD3N","colab_type":"text"},"source":["## Cálculo de Métricas y Conclusiones"]},{"cell_type":"code","metadata":{"id":"vZ8czozFLDK9","colab_type":"code","colab":{}},"source":["# Las métricas solicitadas son: MSE, MAE, RMSE, R Square\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"84UZS4XOugXn","colab_type":"text"},"source":["## Opcional: Aplicar PCA"]},{"cell_type":"code","metadata":{"id":"Fa3PWpcj-fva","colab_type":"code","colab":{}},"source":["from sklearn.decomposition import PCA\n","n = \n","pca = PCA(n_components=n)\n","reduc_dim = pca.fit(X)\n","reduc_dim.components_\n","reduc_dim.explained_variance_"],"execution_count":null,"outputs":[]}]}